{"cells":[{"cell_type":"markdown","metadata":{"id":"Jeo3-FO-6fJm"},"source":["# Policy Gradient Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"GNsuE0M_7aAN"},"source":["### **Team Member:** 108024507 張文騰 / 108024512 吳紹丞 / 108024519 劉怡禎 / 109062659 蘇瑞揚"]},{"cell_type":"markdown","metadata":{"id":"3c_HHsUL70tk"},"source":["Below, we divide our training code into 3 parts which are PPO, A3C, and PG. And we have tried framed-based RL based on PPO."]},{"cell_type":"markdown","metadata":{"id":"mXC676Wh7c06"},"source":["# **Content**\r\n","#### 0. **Game Statistics**\r\n","        - EDA result\r\n","#### 1. **PPO** \r\n","        - State-Based\r\n","#### 2. **A3C**\r\n","        - State-Based\r\n","#### 3. **PG**\r\n","        - State-Based\r\n","#### 4. **Conclusion**"]},{"cell_type":"markdown","metadata":{"id":"vIgSeuma8Pbh"},"source":["# Set up Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ecNjvmEJ6fJo","outputId":"f502fd2f-7901-41cf-ca0c-5fc0e541c93b"},"outputs":[{"data":{"text/plain":["'3.7.9 (default, Aug 31 2020, 12:42:55) \\n[GCC 7.3.0]'"]},"execution_count":2,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["import sys\n","sys.version"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ik2Az0d_6fJp","outputId":"01d7bc5c-674e-4f53-a308-6d4c301d2361"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/ingmember03/DL2020/DL2020_07/comp4\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","\n","print(os.getcwd())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLE8NcBU6fJp"},"outputs":[],"source":["# limit the uasge of memory\n","os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        tf.config.experimental.set_virtual_device_configuration(\n","            gpus[0],\n","            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=500)])\n","    except RuntimeError as e:\n","        print(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjxJufGP6fJq","outputId":"cb1d6992-0d16-413b-e87c-9b5f82209c52"},"outputs":[{"name":"stdout","output_type":"stream","text":["pygame 1.9.6\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n","couldn't import doomish\n","Couldn't import doom\n"]}],"source":["os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"  # this line make pop-out window not appear\n","from ple.games.flappybird import FlappyBird\n","from ple import PLE\n","\n","game = FlappyBird()\n","env = PLE(game, fps=30, display_screen=False)  # environment interface to game\n","env.reset_game()"]},{"cell_type":"markdown","metadata":{"id":"OZ0o7HMMJdLF"},"source":["# 0. Game Statistics"]},{"cell_type":"markdown","metadata":{"id":"_ha9dYcI6fJx"},"source":["#### Game EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nI3sYD516fJy"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from scipy import ndimage\n","\n","env.reset_game()\n","T = 0\n","rewards = []\n","frames = []\n","states = []"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtwmhHhR6fJy"},"outputs":[],"source":["# to pass first pipe\n","# 0 -> 38 times -> 7\n","# 1 -> 22 times -> go \n","\n","rewards.append(env.act(env.getActionSet()[0]))\n","states.append(TA_state())\n","print(game.getGameState())\n","frames.append(env.getScreenRGB())\n","plt.imshow(ndimage.rotate(frames[T], 270))\n","print(\"\\nT: {} REWARD: {}\".format(T, rewards[T]))\n","\n","T += 1\n","\n","rewards.append(env.act(env.getActionSet()[1]))\n","states.append(TA_state())\n","print(game.getGameState())\n","frames.append(env.getScreenRGB())\n","plt.imshow(ndimage.rotate(frames[T], 270))\n","print(\"\\nT: {} REWARD: {}\".format(T, rewards[T]))\n","\n","T += 1"]},{"cell_type":"markdown","metadata":{"id":"41xI1TZr6fJy"},"source":["1. T = 61 -> REWARD = -5\n","2. T = 97 -> REWARD = -5\n","3. T = 18 -> Earliest Dead  \n","4. if player_y > 390 -> die\n","5. if player_y < -5 -> die\n","6. next_pipe_dist_to_player is the distance between player and the exit point of pipe, which means if one hits the pipe directly at first, there are still 61 units away from the exit point of pipe.\n","\n","**Conclusion:**\n"," - Reward function should weigh up while passing thru the pipe, the inequality form is 61 <= next_pipe_dist_to_player <= 0"]},{"cell_type":"markdown","metadata":{"id":"p_2MgaBrJj2x"},"source":["# 1. PPO - state-based"]},{"cell_type":"markdown","metadata":{"id":"VZtzkpwF6fJq"},"source":["### Define Make Movie Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMXgr-_96fJq"},"outputs":[],"source":["import moviepy.editor as mpy\n","\n","def make_anim(images, fps=60, true_image=False):\n","    duration = len(images) / fps\n","\n","    def make_frame(t):\n","        try:\n","            x = images[int(len(images) / duration * t)]\n","        except:\n","            x = images[-1]\n","\n","        if true_image:\n","            return x.astype(np.uint8)\n","        else:\n","            return ((x + 1) / 2 * 255).astype(np.uint8)\n","\n","    clip = mpy.VideoClip(make_frame, duration=duration)\n","    clip.fps = fps\n","    return clip"]},{"cell_type":"markdown","metadata":{"id":"q_iyPlB46fJr"},"source":["### Define Actor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sR1_OwpV6fJr","outputId":"81c1396f-6564-4c22-a348-c7e0878d9bfa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_4 (Dense)              (None, 32)                288       \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 32)                0         \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 32)                1056      \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 64)                2112      \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 2)                 130       \n","_________________________________________________________________\n","softmax_1 (Softmax)          (None, 2)                 0         \n","=================================================================\n","Total params: 3,586\n","Trainable params: 3,586\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["Actor = tf.keras.Sequential()\n","# Actor.add(tf.keras.layers.Dense(32, input_dim = 8, activation='relu',kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform'))\n","Actor.add(tf.keras.layers.Dense(32, input_dim = 8, kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform'))\n","Actor.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n","Actor.add(tf.keras.layers.Dense(32, kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform'))\n","Actor.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n","Actor.add(tf.keras.layers.Dense(64,  kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform'))\n","Actor.add(tf.keras.layers.LeakyReLU(alpha=0.3))\n","Actor.add(tf.keras.layers.Dense(2,kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform'))\n","Actor.add(tf.keras.layers.Softmax())\n","Actor.build()\n","Actor_opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","print(Actor.summary())"]},{"cell_type":"markdown","metadata":{"id":"M107PKOf6fJs"},"source":["### TA_state"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Ia9Ia-t6fJs"},"outputs":[],"source":["import copy\n","\n","def TA_state():\n","    state = copy.deepcopy(game.getGameState())\n","    \n","    state['next_next_pipe_bottom_y'] -= state['player_y']\n","    state['next_next_pipe_top_y'] -= state['player_y']\n","    state['next_pipe_bottom_y'] -= state['player_y']\n","    state['next_pipe_top_y'] -= state['player_y']\n","    relative_state = list(state.values())\n","\n","\n","    # # return the state in tensor type, with batch dimension\n","    # relative_state = tf.convert_to_tensor(relative_state, dtype=tf.float32)\n","    # relative_state = tf.expand_dims(relative_state, axis=0)\n","    \n","    return relative_state"]},{"cell_type":"markdown","metadata":{"id":"05hwVC_x6fJt"},"source":["### Define Forward Advantage Function (Not Efficient)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQamC1SB6fJt"},"outputs":[],"source":["def Advantage_func(rewards, time, gamma):\n","    dis_rewards = []\n","    for t in range(1,time+1,1):\n","        t_prime = t\n","        dis_reward = 0\n","        count = t - 1\n","        for i in range(count,len(rewards),1):\n","            dis_reward += rewards[i] * (gamma ** (t_prime-t))\n","            t_prime += 1\n","        dis_rewards.append(dis_reward)\n","    \n","    # naive baseline\n","    # baseline = np.mean(dis_rewards)\n","    # Advantage = dis_rewards-baseline\n","    Advantage = dis_rewards\n","    return (Advantage)"]},{"cell_type":"markdown","metadata":{"id":"AamX9Jwa6fJt"},"source":["### Define Backward Advantage Function (Efficient)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgL_lka-6fJu"},"outputs":[],"source":["def Advantage_func_fromback(rewards, time, gamma):\n","    dis_rewards = []\n","    dis_reward = 0\n","    count = 0\n","\n","    for t in range(time,0,-1):\n","        dis_reward = dis_reward * gamma + rewards[t-1]\n","            \n","        dis_rewards.append(dis_reward)\n","        count += 1\n","    \n","    # naive baseline\n","    # baseline = np.mean(dis_rewards)\n","    # Advantage = dis_rewards-baseline\n","    Advantage = list(reversed(dis_rewards))\n","    return (Advantage)"]},{"cell_type":"markdown","metadata":{"id":"gq5SUb-KA8t1"},"source":["#### Check the result of two advantage functions (should be the same)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQ8eBUdC6fJu","outputId":"d97dad7c-06cc-475f-e216-c9ec6a59b275"},"outputs":[{"name":"stdout","output_type":"stream","text":["[-0.06909741  1.58449479  0.89380297  0.76317422  1.11642288]\n","[3.0468995689243488, 3.4622188605162063, 2.0863600803735247, 1.3250634612221566, 0.6243213826610516]\n","[3.0468995689243443, 3.4622188605162036, 2.086360080373523, 1.3250634612221557, 0.624321382661053]\n"]}],"source":["r = np.random.normal(size = 1000, loc = 0, scale = 1)\n","print(r[:5])\n","print(Advantage_func(r, len(r), 0.9)[:5]) # as size goes large, it'll take too much time\n","print(Advantage_func_fromback(r, len(r), 0.9)[:5])"]},{"cell_type":"markdown","metadata":{"id":"gpErlGvH6fJv"},"source":["### Define Loss Function (-Objective Function) with For Loop (Not Efficient)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1IUJclf6fJv"},"outputs":[],"source":["def J_func(probs, old_probs, adv, epsilon):\n","    J = []\n","    for up, op, a in zip(probs, old_probs, adv):\n","        p_ratio = up/op\n","        s1 = (p_ratio * a)\n","        s2 =  (tf.clip_by_value(p_ratio, 1-epsilon, 1+epsilon) * a)\n","        J.append(-tf.math.minimum(s1,s2))\n","\n","    return(J)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4taXB22C6fJw","outputId":"64773497-617d-4616-819e-435d3fb93988"},"outputs":[{"name":"stdout","output_type":"stream","text":["[90.49000000000001, 56.1, 29.0, 10.0]\n"]},{"data":{"text/plain":["[<tf.Tensor: shape=(), dtype=float32, numpy=-95.014496>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=-58.904995>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=-29.0>,\n"," <tf.Tensor: shape=(), dtype=float32, numpy=-7.142857>]"]},"execution_count":17,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["r = [40,30,20,10]\n","adv = Advantage_func_fromback(r, len(r), 0.9)\n","print(adv)\n","probs = [0.8,0.7,0.6,0.5]\n","old_probs = [0.7,0.6,0.6,0.7]\n","\n","J_func(probs, old_probs, adv, 0.05)"]},{"cell_type":"markdown","metadata":{"id":"9TVuVu1-6fJw"},"source":["### Define Loss Function (-Objective Function) with TF API (Efficient)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewUifGVm6fJw"},"outputs":[],"source":["def J_func_tf(probs, old_probs, adv, epsilon):\n","\n","    p_ratio = tf.divide(probs,old_probs)\n","    s1 = tf.multiply(p_ratio,tf.cast(adv, dtype = tf.float32))\n","    s2 = tf.multiply(tf.clip_by_value(p_ratio, 1-epsilon, 1+epsilon), tf.cast(adv, dtype = tf.float32))\n","    J = -tf.math.minimum(s1,s2)\n","\n","    return(J)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0TBKxxqp6fJw","outputId":"e8c53991-98fe-46a3-8a02-1e763f9827b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["[90.49000000000001, 56.1, 29.0, 10.0]\n"]},{"data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-95.014496 , -58.904995 , -29.       ,  -7.1428576], dtype=float32)>"]},"execution_count":19,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["r = [40,30,20,10]\n","adv = Advantage_func_fromback(r, len(r), 0.9)\n","print(adv)\n","probs = tf.constant([0.8,0.7,0.6,0.5])\n","old_probs = tf.constant([0.7,0.6,0.6,0.7])\n","adv = tf.constant(adv)\n","\n","J_func_tf(probs, old_probs, adv, 0.05)"]},{"cell_type":"markdown","metadata":{"id":"XhdOpRbL6fJx"},"source":["### Define Learning Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X0W2cvi6fJx"},"outputs":[],"source":["EPSILON = 0.1\n","\n","def train_step(states, actions, adv, probs, ep):\n","\n","    with tf.GradientTape() as tape:\n","        prs = []\n","        pr = Agent(states)\n","\n","        # print(pr)\n","        for idx, a in enumerate(actions):\n","            prs.append(pr[idx][a])\n","\n","        prs = tf.stack(prs, axis = 0)\n","        probs = tf.constant(probs)\n","        adv = tf.stack(adv, axis = 0)\n","\n","        EXP_J = J_func_tf(prs, probs, adv, EPSILON)\n","        actor_loss = (tf.math.reduce_mean(EXP_J))\n","        # print(\"P: \", prs,'\\n')\n","        # print(\"loss: \", actor_loss,'\\n')\n","        \n","    grads = tape.gradient(actor_loss, Agent.trainable_variables)\n","    Actor_opt.apply_gradients(zip(grads, Agent.trainable_variables))\n","\n","    return actor_loss"]},{"cell_type":"markdown","metadata":{"id":"sO01NKZC6fJz"},"source":["#### Define Reward Function - Add some rewards and penalties thru the road to pipe\r\n"," - When 141 >= next_distance_to_pipe > 61, if bird is higher or lower than the top or bottom of the pipe, penalty = -0.5; if between, reward = 1.5\r\n"," - When 61 >=  next_distance_to_pipe > 0, if bird is higher or lower than the top or bottom of the pipe, penalty = 0; if between, reward = 2.5\r\n"," - Note that the original reward and penalty in game are 1 and -5 when bird flies through the pipe or die\r\n","\r\n","We add additional rewards criteria (first 2 bullet points) onto the original rewards in PPO."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnJkYHJF6fJz"},"outputs":[],"source":["REWARD_DIST = 20\n","\n","def Reward_func(states, rewards):\n","\n","    Rewards_adj = []\n","\n","    for i in range(1, len(states), 1):\n","        \n","        next_state = states[i][0]\n","        \n","        if next_state[2] > 61 and next_state[2] <= 61 + 4 * REWARD_DIST:\n","            if next_state[0] <= next_state[3]+next_state[0]:\n","                re = -0.5\n","            elif next_state[0] >= next_state[4]+next_state[0]:\n","                re = -0.5\n","            else:\n","                re = 1.5 # randomly assign a value\n","        # the minimum of next_pipe_distance = 1\n","        elif next_state[2] <= 61 and next_state[2] > 0:\n","            if next_state[0] <= next_state[3]+next_state[0]:\n","                re = 0\n","            elif next_state[0] >= next_state[4]+next_state[0]:\n","                re = 0\n","            else:\n","                re = 2.5 # randomly assign a value\n","        else:\n","            re = 0\n","\n","        re = tf.dtypes.cast(re, tf.float32)\n","        Rewards_adj.append(re)\n","\n","    Rewards_all = tf.constant(rewards) + Rewards_adj \n","\n","    return Rewards_all"]},{"cell_type":"markdown","metadata":{"id":"_3SgCtG06fJz"},"source":["#### Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ehsDWmK6fJ0"},"outputs":[],"source":["# model_path = \"./model_rewardsum442/\"\n","# Agent = tf.keras.models.load_model(model_path, compile=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ev7oabp_6fJ0","outputId":"0d5c155f-ab29-4cc3-d1c8-a2e11e497d78","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["EPISODE: 0\n","WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[0] epochs: 0\n","LOSS: 4.6023359298706055\n","[0] epochs: 1\n","LOSS: 4.57857608795166\n","[0] epochs: 2\n","LOSS: 4.560789108276367\n","[0] epochs: 3\n","LOSS: 4.556968688964844\n","[0] epochs: 4\n","LOSS: 4.5563225746154785\n","[0] epochs: 5\n","LOSS: 4.547998428344727\n","[0] epochs: 6\n","LOSS: 4.5424604415893555\n","[0] epochs: 7\n","LOSS: 4.540435314178467\n","[0] epochs: 8\n","LOSS: 4.540755748748779\n","[0] epochs: 9\n","LOSS: 4.5441484451293945\n","time_live: 61\n","cumulated reward: -15.0\n","avg_time_live: 61.0\n","avg_cum_reward: -15.0\n","max_time_live: 61\n","max_cum_reward: -15.0\n","\n","EPISODE: 1\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[1] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 57.0\n","avg_cum_reward: -13.0\n","max_time_live: 61\n","max_cum_reward: -11.0\n","\n","EPISODE: 2\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[2] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 57.0\n","avg_cum_reward: -12.833000183105469\n","max_time_live: 61\n","max_cum_reward: -11.0\n","\n","EPISODE: 3\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[3] epochs: 0\n","LOSS: -1.2809418439865112\n","time_live: 58\n","cumulated reward: 4.5\n","avg_time_live: 57.0\n","avg_cum_reward: -8.5\n","max_time_live: 61\n","max_cum_reward: 4.5\n","\n","EPISODE: 4\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[4] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 57.0\n","avg_cum_reward: -9.300000190734863\n","max_time_live: 61\n","max_cum_reward: 4.5\n","\n","EPISODE: 5\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[5] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 55.0\n","avg_cum_reward: -8.833000183105469\n","max_time_live: 61\n","max_cum_reward: 4.5\n","\n","EPISODE: 6\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[6] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 55.0\n","avg_cum_reward: -9.21399974822998\n","max_time_live: 61\n","max_cum_reward: 4.5\n","\n","EPISODE: 7\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[7] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 55.0\n","avg_cum_reward: -9.687999725341797\n","max_time_live: 61\n","max_cum_reward: 4.5\n","\n","EPISODE: 8\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[8] epochs: 0\n","LOSS: 1.0571132898330688\n","[8] epochs: 1\n","LOSS: 1.0574713945388794\n","[8] epochs: 2\n","LOSS: 1.0576541423797607\n","[8] epochs: 3\n","LOSS: 1.0576847791671753\n","[8] epochs: 4\n","LOSS: 1.0575575828552246\n","[8] epochs: 5\n","LOSS: 1.0572984218597412\n","[8] epochs: 6\n","LOSS: 1.0569941997528076\n","[8] epochs: 7\n","LOSS: 1.05660879611969\n","[8] epochs: 8\n","LOSS: 1.0561412572860718\n","[8] epochs: 9\n","LOSS: 1.0555639266967773\n","time_live: 62\n","cumulated reward: -3.0\n","avg_time_live: 56.0\n","avg_cum_reward: -8.944000244140625\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 9\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[9] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 55.0\n","avg_cum_reward: -8.899999618530273\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 10\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[10] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 55.0\n","avg_cum_reward: -9.27299976348877\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 11\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[11] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 55.0\n","avg_cum_reward: -9.416999816894531\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 12\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[12] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 55.0\n","avg_cum_reward: -9.654000282287598\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 13\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[13] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -9.428999900817871\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 14\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[14] epochs: 0\n","LOSS: 4.426576614379883\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 55.0\n","avg_cum_reward: -9.732999801635742\n","max_time_live: 62\n","max_cum_reward: 4.5\n","\n","EPISODE: 15\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(60,), dtype=float32)\n","[15] epochs: 0\n","LOSS: -2.150681972503662\n","time_live: 60\n","cumulated reward: 7.5\n","avg_time_live: 55.0\n","avg_cum_reward: -8.656000137329102\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 16\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[16] epochs: 0\n","LOSS: 0.4599873125553131\n","time_live: 57\n","cumulated reward: -1.0\n","avg_time_live: 55.0\n","avg_cum_reward: -8.206000328063965\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 17\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[17] epochs: 0\n","LOSS: -1.3194698095321655\n","time_live: 62\n","cumulated reward: 5.0\n","avg_time_live: 55.0\n","avg_cum_reward: -7.4720001220703125\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 18\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[18] epochs: 0\n","LOSS: 3.418013334274292\n","time_live: 61\n","cumulated reward: -11.0\n","avg_time_live: 56.0\n","avg_cum_reward: -7.6579999923706055\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 19\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[19] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 56.0\n","avg_cum_reward: -7.925000190734863\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 20\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[20] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 55.0\n","avg_cum_reward: -7.88100004196167\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 21\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[21] epochs: 0\n","LOSS: 2.217240810394287\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.75\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 22\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[22] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.673999786376953\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 23\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0. -5.], shape=(38,), dtype=float32)\n","[23] epochs: 0\n","LOSS: 2.256852865219116\n","time_live: 38\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.561999797821045\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 24\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[24] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.860000133514404\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 25\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[25] epochs: 0\n","LOSS: 2.3480770587921143\n","time_live: 52\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.808000087738037\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 26\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[26] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.741000175476074\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 27\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[27] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -8.0\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 28\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[28] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.034000396728516\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 29\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[29] epochs: 0\n","LOSS: 4.426576137542725\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 54.0\n","avg_cum_reward: -8.232999801635742\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 30\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[30] epochs: 0\n","LOSS: 3.7125649452209473\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 54.0\n","avg_cum_reward: -8.305999755859375\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 31\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[31] epochs: 0\n","LOSS: 4.33444356918335\n","time_live: 58\n","cumulated reward: -13.5\n","avg_time_live: 54.0\n","avg_cum_reward: -8.468999862670898\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 32\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[32] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -8.666999816894531\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 33\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[33] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 54.0\n","avg_cum_reward: -8.588000297546387\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 34\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[34] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 54.0\n","avg_cum_reward: -8.670999526977539\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 35\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[35] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.569000244140625\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 36\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[36] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.743000030517578\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 37\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[37] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.776000022888184\n","max_time_live: 62\n","max_cum_reward: 7.5\n","\n","EPISODE: 38\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[38] epochs: 0\n","LOSS: -2.3529272079467773\n","time_live: 59\n","cumulated reward: 8.0\n","avg_time_live: 54.0\n","avg_cum_reward: -8.345999717712402\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 39\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[39] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.362000465393066\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 40\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[40] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -8.463000297546387\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 41\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[41] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 54.0\n","avg_cum_reward: -8.53600025177002\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 42\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[42] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -8.557999610900879\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 43\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[43] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.590999603271484\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 44\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[44] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.57800006866455\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 45\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[45] epochs: 0\n","LOSS: 4.426576137542725\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -8.696000099182129\n","max_time_live: 62\n","max_cum_reward: 8.0\n","\n","EPISODE: 46\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  2.5  0.  -4. ], shape=(66,), dtype=float32)\n","[46] epochs: 0\n","LOSS: -9.426312446594238\n","[46] epochs: 1\n","LOSS: -9.433876991271973\n","[46] epochs: 2\n","LOSS: -9.451379776000977\n","[46] epochs: 3\n","LOSS: -9.479188919067383\n","[46] epochs: 4\n","LOSS: -9.519675254821777\n","[46] epochs: 5\n","LOSS: -9.573171615600586\n","[46] epochs: 6\n","LOSS: -9.5424165725708\n","[46] epochs: 7\n","LOSS: -9.626690864562988\n","[46] epochs: 8\n","LOSS: -9.618765830993652\n","[46] epochs: 9\n","LOSS: -9.615835189819336\n","time_live: 66\n","cumulated reward: 33.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.797999858856201\n","max_time_live: 66\n","max_cum_reward: 33.5\n","\n","EPISODE: 47\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[47] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.947999954223633\n","max_time_live: 66\n","max_cum_reward: 33.5\n","\n","EPISODE: 48\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[48] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.980000019073486\n","max_time_live: 66\n","max_cum_reward: 33.5\n","\n","EPISODE: 49\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  2.5  2.5  3.5  2.5  2.5  0.  -5. ], shape=(70,), dtype=float32)\n","[49] epochs: 0\n","LOSS: -8.993626594543457\n","[49] epochs: 1\n","LOSS: -9.039453506469727\n","[49] epochs: 2\n","LOSS: -9.101424217224121\n","[49] epochs: 3\n","LOSS: -9.1288423538208\n","[49] epochs: 4\n","LOSS: -9.117047309875488\n","[49] epochs: 5\n","LOSS: -9.163228034973145\n","[49] epochs: 6\n","LOSS: -9.154006004333496\n","[49] epochs: 7\n","LOSS: -9.139143943786621\n","[49] epochs: 8\n","LOSS: -9.147631645202637\n","[49] epochs: 9\n","LOSS: -9.167559623718262\n","time_live: 70\n","cumulated reward: 33.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.150000095367432\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 50\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[50] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.196000099182129\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 51\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[51] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.191999912261963\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 52\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[52] epochs: 0\n","LOSS: 2.7945144176483154\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.189000129699707\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 53\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[53] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.333000183105469\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 54\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[54] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.317999839782715\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 55\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[55] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.383999824523926\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 56\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[56] epochs: 0\n","LOSS: -3.1271560192108154\n","time_live: 62\n","cumulated reward: 11.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.060999870300293\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 57\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[57] epochs: 0\n","LOSS: -4.342574596405029\n","time_live: 62\n","cumulated reward: 15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -6.681000232696533\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 58\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[58] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -6.677999973297119\n","max_time_live: 70\n","max_cum_reward: 33.5\n","\n","EPISODE: 59\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  2.5  2.5  3.5  2.5  2.5  2.5  2.5\n","  0.  -5. ], shape=(72,), dtype=float32)\n","[59] epochs: 0\n","LOSS: -12.663830757141113\n","[59] epochs: 1\n","LOSS: -12.648165702819824\n","[59] epochs: 2\n","LOSS: -12.664101600646973\n","[59] epochs: 3\n","LOSS: -12.70400619506836\n","[59] epochs: 4\n","LOSS: -12.722103118896484\n","[59] epochs: 5\n","LOSS: -12.743592262268066\n","[59] epochs: 6\n","LOSS: -12.721006393432617\n","[59] epochs: 7\n","LOSS: -12.706498146057129\n","[59] epochs: 8\n","LOSS: -12.715510368347168\n","[59] epochs: 9\n","LOSS: -12.74697494506836\n","time_live: 72\n","cumulated reward: 48.5\n","avg_time_live: 54.0\n","avg_cum_reward: -5.757999897003174\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 60\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[60] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -5.745999813079834\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 61\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[61] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 54.0\n","avg_cum_reward: -5.806000232696533\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 62\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[62] epochs: 0\n","LOSS: 2.481861114501953\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 54.0\n","avg_cum_reward: -5.809999942779541\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 63\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[63] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -5.796999931335449\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 64\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[64] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -5.830999851226807\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 65\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[65] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -5.932000160217285\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 66\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[66] epochs: 0\n","LOSS: 2.217240810394287\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -5.918000221252441\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 67\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(32,), dtype=float32)\n","[67] epochs: 0\n","LOSS: 2.519650936126709\n","time_live: 32\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -5.9039998054504395\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 68\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[68] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -5.956999778747559\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 69\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[69] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -5.9710001945495605\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 70\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[70] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.063000202178955\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 71\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[71] epochs: 0\n","LOSS: 3.7125649452209473\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.125\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 72\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[72] epochs: 0\n","LOSS: 3.712564706802368\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.184999942779541\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 73\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[73] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.209000110626221\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 74\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[74] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.28000020980835\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 75\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(46,), dtype=float32)\n","[75] epochs: 0\n","LOSS: 2.94142746925354\n","time_live: 46\n","cumulated reward: -7.5\n","avg_time_live: 52.0\n","avg_cum_reward: -6.296000003814697\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 76\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[76] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 52.0\n","avg_cum_reward: -6.2789998054504395\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 77\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[77] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 52.0\n","avg_cum_reward: -6.353000164031982\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 78\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[78] epochs: 0\n","LOSS: 1.659364938735962\n","time_live: 52\n","cumulated reward: -4.5\n","avg_time_live: 52.0\n","avg_cum_reward: -6.328999996185303\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 79\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[79] epochs: 0\n","LOSS: 3.7125649452209473\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 52.0\n","avg_cum_reward: -6.38100004196167\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 80\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[80] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 52.0\n","avg_cum_reward: -6.48799991607666\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 81\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[81] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.591000080108643\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 82\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[82] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.692999839782715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 83\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[83] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.75600004196167\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 84\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[84] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.853000164031982\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 85\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[85] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.947999954223633\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 86\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[86] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.039999961853027\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 87\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[87] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.067999839782715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 88\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[88] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.124000072479248\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 89\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[89] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.189000129699707\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 90\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[90] epochs: 0\n","LOSS: -6.179171562194824\n","time_live: 62\n","cumulated reward: 21.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.879000186920166\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 91\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[91] epochs: 0\n","LOSS: 4.426576137542725\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.956999778747559\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 92\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[92] epochs: 0\n","LOSS: 1.1311715841293335\n","time_live: 53\n","cumulated reward: -3.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.914000034332275\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 93\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[93] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.914999961853027\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 94\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[94] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.0\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 95\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[95] epochs: 0\n","LOSS: 0.6203011870384216\n","time_live: 58\n","cumulated reward: -1.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.942999839782715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 96\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[96] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.026000022888184\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 97\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[97] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.040999889373779\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 98\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[98] epochs: 0\n","LOSS: 3.7125649452209473\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.076000213623047\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 99\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[99] epochs: 0\n","LOSS: 4.6023359298706055\n","time_live: 61\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.15500020980835\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 100\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[100] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.144000053405762\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 101\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[101] epochs: 0\n","LOSS: 2.620197057723999\n","time_live: 54\n","cumulated reward: -7.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.146999835968018\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 102\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[102] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.175000190734863\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 103\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[103] epochs: 0\n","LOSS: 2.1787192821502686\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.1539998054504395\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 104\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[104] epochs: 0\n","LOSS: 4.426576137542725\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.218999862670898\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 105\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[105] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.291999816894531\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 106\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[106] epochs: 0\n","LOSS: -3.7722408771514893\n","time_live: 62\n","cumulated reward: 13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.103000164031982\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 107\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[107] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.116000175476074\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 108\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[108] epochs: 0\n","LOSS: 2.228588819503784\n","time_live: 62\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.114999771118164\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 109\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[109] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.185999870300293\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 110\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(46,), dtype=float32)\n","[110] epochs: 0\n","LOSS: 2.94142746925354\n","time_live: 46\n","cumulated reward: -7.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.189000129699707\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 111\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[111] epochs: 0\n","LOSS: 3.724776268005371\n","time_live: 58\n","cumulated reward: -11.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.228000164031982\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 112\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[112] epochs: 0\n","LOSS: 3.3872241973876953\n","time_live: 62\n","cumulated reward: -11.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.261000156402588\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 113\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  2.5  2.5  3.5  0.  -5. ], shape=(68,), dtype=float32)\n","[113] epochs: 0\n","LOSS: -10.56368637084961\n","time_live: 68\n","cumulated reward: 38.5\n","avg_time_live: 54.0\n","avg_cum_reward: -6.860000133514404\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 114\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[114] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -6.929999828338623\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 115\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[115] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.0\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 116\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[116] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.038000106811523\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 117\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[117] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.105999946594238\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 118\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[118] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.11299991607666\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 119\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[119] epochs: 0\n","LOSS: 3.9613001346588135\n","time_live: 62\n","cumulated reward: -13.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.1620001792907715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 120\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[120] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.169000148773193\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 121\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[121] epochs: 0\n","LOSS: -3.7339060306549072\n","time_live: 62\n","cumulated reward: 13.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.004000186920166\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 122\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[122] epochs: 0\n","LOSS: 4.426576137542725\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.060999870300293\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 123\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[123] epochs: 0\n","LOSS: 3.3872241973876953\n","time_live: 62\n","cumulated reward: -11.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.0929999351501465\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 124\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[124] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.156000137329102\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 125\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[125] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.138999938964844\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 126\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[126] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.138000011444092\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 127\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[127] epochs: 0\n","LOSS: 2.217240810394287\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.120999813079834\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 128\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[128] epochs: 0\n","LOSS: 4.426576614379883\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.173999786376953\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 129\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[129] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.235000133514404\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 130\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[130] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.294000148773193\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 131\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[131] epochs: 0\n","LOSS: 3.712564706802368\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.317999839782715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 132\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[132] epochs: 0\n","LOSS: -1.9198293685913086\n","time_live: 62\n","cumulated reward: 7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.210999965667725\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 133\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[133] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.269000053405762\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 134\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[134] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.289000034332275\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 135\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[135] epochs: 0\n","LOSS: -1.9198293685913086\n","time_live: 62\n","cumulated reward: 7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.184000015258789\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 136\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[136] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.241000175476074\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 137\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[137] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.238999843597412\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 138\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[138] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.251999855041504\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 139\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[139] epochs: 0\n","LOSS: 3.279320001602173\n","time_live: 54\n","cumulated reward: -9.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.26800012588501\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 140\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[140] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.265999794006348\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 141\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[141] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.2779998779296875\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 142\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[142] epochs: 0\n","LOSS: 4.6023359298706055\n","time_live: 61\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.331999778747559\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 143\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0. -5.], shape=(38,), dtype=float32)\n","[143] epochs: 0\n","LOSS: 2.256852865219116\n","time_live: 38\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.315999984741211\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 144\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[144] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.3480000495910645\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 145\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[145] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.401000022888184\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 146\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[146] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.394999980926514\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 147\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[147] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.446000099182129\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 148\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[148] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.46999979019165\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 149\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[149] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.493000030517578\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 150\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0. -5.], shape=(38,), dtype=float32)\n","[150] epochs: 0\n","LOSS: 2.256852865219116\n","time_live: 38\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.4770002365112305\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 151\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[151] epochs: 0\n","LOSS: -5.034762382507324\n","time_live: 62\n","cumulated reward: 17.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.315999984741211\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 152\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[152] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.320000171661377\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 153\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," -5.], shape=(37,), dtype=float32)\n","[153] epochs: 0\n","LOSS: 2.297593355178833\n","time_live: 37\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.304999828338623\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 154\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[154] epochs: 0\n","LOSS: -5.608837604522705\n","time_live: 62\n","cumulated reward: 19.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.135000228881836\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 155\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[155] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.13100004196167\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 156\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[156] epochs: 0\n","LOSS: 3.9341485500335693\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.158999919891357\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 157\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[157] epochs: 0\n","LOSS: 4.33444356918335\n","time_live: 58\n","cumulated reward: -13.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.198999881744385\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 158\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[158] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.210999965667725\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 159\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(60,), dtype=float32)\n","[159] epochs: 0\n","LOSS: 4.515830039978027\n","time_live: 60\n","cumulated reward: -14.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.25600004196167\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 160\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[160] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.303999900817871\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 161\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[161] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.335999965667725\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 162\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[162] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.382999897003174\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 163\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[163] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.429999828338623\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 164\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[164] epochs: 0\n","LOSS: 3.5957324504852295\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.445000171661377\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 165\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[165] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.491000175476074\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 166\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[166] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.47599983215332\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 167\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[167] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.4730000495910645\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 168\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[168] epochs: 0\n","LOSS: 4.239290237426758\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.50600004196167\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 169\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[169] epochs: 0\n","LOSS: 0.775385856628418\n","time_live: 59\n","cumulated reward: -2.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.473999977111816\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 170\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5\n"," -0.5 -0.5 -0.5  1.5  1.5  0.  -5. ], shape=(63,), dtype=float32)\n","[170] epochs: 0\n","LOSS: -4.884139060974121\n","time_live: 63\n","cumulated reward: 17.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.329999923706055\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 171\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[171] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.375\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 172\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[172] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.38700008392334\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 173\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[173] epochs: 0\n","LOSS: 0.4669572114944458\n","time_live: 62\n","cumulated reward: -1.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.35099983215332\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 174\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[174] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.3460001945495605\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 175\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[175] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.331999778747559\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 176\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[176] epochs: 0\n","LOSS: 0.2941751778125763\n","time_live: 56\n","cumulated reward: -0.5\n","avg_time_live: 54.0\n","avg_cum_reward: -7.294000148773193\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 177\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[177] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.291999816894531\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 178\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[178] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.307000160217285\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 179\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[179] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.294000148773193\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 180\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[180] epochs: 0\n","LOSS: 0.08573053032159805\n","time_live: 51\n","cumulated reward: 0.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.254000186920166\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 181\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0. -5.], shape=(38,), dtype=float32)\n","[181] epochs: 0\n","LOSS: 2.256852865219116\n","time_live: 38\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.242000102996826\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 182\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(33,), dtype=float32)\n","[182] epochs: 0\n","LOSS: 2.4726483821868896\n","time_live: 33\n","cumulated reward: -5.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.230000019073486\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 183\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[183] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.255000114440918\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 184\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[184] epochs: 0\n","LOSS: 4.6023359298706055\n","time_live: 61\n","cumulated reward: -15.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.296999931335449\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 185\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[185] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 54.0\n","avg_cum_reward: -7.306000232696533\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 186\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[186] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.294000148773193\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 187\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[187] epochs: 0\n","LOSS: 2.481861114501953\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.2870001792907715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 188\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[188] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.328000068664551\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 189\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(33,), dtype=float32)\n","[189] epochs: 0\n","LOSS: 2.4726483821868896\n","time_live: 33\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.315999984741211\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 190\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(36,), dtype=float32)\n","[190] epochs: 0\n","LOSS: 2.3395016193389893\n","time_live: 36\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.303999900817871\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 191\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[191] epochs: 0\n","LOSS: 2.217240810394287\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.291999816894531\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 192\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[192] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.310999870300293\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 193\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[193] epochs: 0\n","LOSS: -4.953069686889648\n","time_live: 62\n","cumulated reward: 17.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.185999870300293\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 194\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[194] epochs: 0\n","LOSS: 0.9443573951721191\n","time_live: 56\n","cumulated reward: -2.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.1620001792907715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 195\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[195] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.165999889373779\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 196\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(60,), dtype=float32)\n","[196] epochs: 0\n","LOSS: 4.515830039978027\n","time_live: 60\n","cumulated reward: -14.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.203000068664551\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 197\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[197] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.197000026702881\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 198\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[198] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.188000202178955\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 199\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[199] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.182000160217285\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 200\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[200] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.197000026702881\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 201\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[201] epochs: 0\n","LOSS: 2.0555038452148438\n","time_live: 50\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.188000202178955\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 202\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[202] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.179999828338623\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 203\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[203] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.171999931335449\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 204\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[204] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.210000038146973\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 205\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[205] epochs: 0\n","LOSS: 2.217240571975708\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.198999881744385\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 206\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[206] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.236999988555908\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 207\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[207] epochs: 0\n","LOSS: 3.0825650691986084\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.239999771118164\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 208\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[208] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.250999927520752\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 209\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[209] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.27400016784668\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 210\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[210] epochs: 0\n","LOSS: 0.46695709228515625\n","time_live: 62\n","cumulated reward: -1.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.24399995803833\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 211\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[211] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.243000030517578\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 212\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[212] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.238999843597412\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 213\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[213] epochs: 0\n","LOSS: 1.6443748474121094\n","time_live: 62\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.229000091552734\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 214\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[214] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.235000133514404\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 215\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0. -5.], shape=(38,), dtype=float32)\n","[215] epochs: 0\n","LOSS: 2.256852865219116\n","time_live: 38\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.224999904632568\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 216\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[216] epochs: 0\n","LOSS: 3.8252992630004883\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.242000102996826\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 217\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[217] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.263999938964844\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 218\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[218] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.263000011444092\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 219\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[219] epochs: 0\n","LOSS: 2.055504083633423\n","time_live: 50\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.255000114440918\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 220\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[220] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.2779998779296875\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 221\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[221] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.296999931335449\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 222\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[222] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.331999778747559\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 223\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[223] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.353000164031982\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 224\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[224] epochs: 0\n","LOSS: 0.44775137305259705\n","time_live: 53\n","cumulated reward: -1.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.323999881744385\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 225\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[225] epochs: 0\n","LOSS: -0.12594930827617645\n","time_live: 62\n","cumulated reward: 1.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.288000106811523\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 226\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[226] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.322000026702881\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 227\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[227] epochs: 0\n","LOSS: 3.712564706802368\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.335999965667725\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 228\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[228] epochs: 0\n","LOSS: 2.873940944671631\n","time_live: 56\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.341000080108643\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 229\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[229] epochs: 0\n","LOSS: 3.3488380908966064\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.3480000495910645\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 230\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[230] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.353000164031982\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 231\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[231] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.386000156402588\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 232\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[232] epochs: 0\n","LOSS: -1.3194698095321655\n","time_live: 62\n","cumulated reward: 5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.333000183105469\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 233\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[233] epochs: 0\n","LOSS: -5.034762382507324\n","time_live: 62\n","cumulated reward: 17.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.229000091552734\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 234\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[234] epochs: 0\n","LOSS: 2.217240571975708\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.218999862670898\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 235\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(31,), dtype=float32)\n","[235] epochs: 0\n","LOSS: 2.5680418014526367\n","time_live: 31\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.210000038146973\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 236\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[236] epochs: 0\n","LOSS: 2.217240571975708\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.199999809265137\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 237\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[237] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.190999984741211\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 238\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[238] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.198999881744385\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 239\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[239] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.190000057220459\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 240\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[240] epochs: 0\n","LOSS: 0.46695709228515625\n","time_live: 62\n","cumulated reward: -1.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.164000034332275\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 241\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[241] epochs: 0\n","LOSS: 4.33444356918335\n","time_live: 58\n","cumulated reward: -13.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.190000057220459\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 242\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[242] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.183000087738037\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 243\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[243] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.192999839782715\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 244\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[244] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.214000225067139\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 245\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[245] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.2170000076293945\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 246\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[246] epochs: 0\n","LOSS: 3.218264579772949\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.2230000495910645\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 247\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0. -5.], shape=(39,), dtype=float32)\n","[247] epochs: 0\n","LOSS: 2.217240810394287\n","time_live: 39\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.214000225067139\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 248\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[248] epochs: 0\n","LOSS: 4.33444356918335\n","time_live: 58\n","cumulated reward: -13.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.238999843597412\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 249\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[249] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.23199987411499\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 250\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[250] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.236999988555908\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 251\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[251] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.242000102996826\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 252\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(46,), dtype=float32)\n","[252] epochs: 0\n","LOSS: 2.94142746925354\n","time_live: 46\n","cumulated reward: -7.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.243000030517578\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 253\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[253] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.242000102996826\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 254\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[254] epochs: 0\n","LOSS: -0.7327024936676025\n","time_live: 54\n","cumulated reward: 2.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.203999996185303\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 255\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[255] epochs: 0\n","LOSS: -1.954888939857483\n","time_live: 61\n","cumulated reward: 7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.1479997634887695\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 256\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[256] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.139999866485596\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 257\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[257] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.1570000648498535\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 258\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[258] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.165999889373779\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 259\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[259] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 52.0\n","avg_cum_reward: -7.163000106811523\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 260\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[260] epochs: 0\n","LOSS: 4.33444356918335\n","time_live: 58\n","cumulated reward: -13.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.188000202178955\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 261\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[261] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.197999954223633\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 262\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[262] epochs: 0\n","LOSS: 3.712564706802368\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.210999965667725\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 263\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[263] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 52.0\n","avg_cum_reward: -7.204999923706055\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 264\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[264] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 52.0\n","avg_cum_reward: -7.209000110626221\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 265\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[265] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 52.0\n","avg_cum_reward: -7.238999843597412\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 266\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[266] epochs: 0\n","LOSS: 3.0325517654418945\n","time_live: 52\n","cumulated reward: -8.5\n","avg_time_live: 52.0\n","avg_cum_reward: -7.243000030517578\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 267\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  0.  -5. ], shape=(65,), dtype=float32)\n","[267] epochs: 0\n","LOSS: -4.093818187713623\n","time_live: 65\n","cumulated reward: 14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.164000034332275\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 268\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[268] epochs: 0\n","LOSS: -1.9368876218795776\n","time_live: 62\n","cumulated reward: 7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.111999988555908\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 269\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[269] epochs: 0\n","LOSS: 4.426576614379883\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.13700008392334\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 270\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(35,), dtype=float32)\n","[270] epochs: 0\n","LOSS: 2.3826184272766113\n","time_live: 35\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.129000186920166\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 271\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[271] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.1579999923706055\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 272\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(33,), dtype=float32)\n","[272] epochs: 0\n","LOSS: 2.4726483821868896\n","time_live: 33\n","cumulated reward: -5.0\n","avg_time_live: 52.0\n","avg_cum_reward: -7.150000095367432\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 273\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[273] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.178999900817871\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 274\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[274] epochs: 0\n","LOSS: 3.9269063472747803\n","time_live: 62\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.199999809265137\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 275\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[275] epochs: 0\n","LOSS: 3.9341485500335693\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 53.0\n","avg_cum_reward: -7.216000080108643\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 276\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n"," -5.], shape=(37,), dtype=float32)\n","[276] epochs: 0\n","LOSS: 2.297593355178833\n","time_live: 37\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.208000183105469\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 277\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[277] epochs: 0\n","LOSS: -1.919829249382019\n","time_live: 62\n","cumulated reward: 7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -7.156000137329102\n","max_time_live: 72\n","max_cum_reward: 48.5\n","\n","EPISODE: 278\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  2.5  2.5  3.5  2.5  2.5  2.5  2.5\n","  2.5  0.  -5. ], shape=(73,), dtype=float32)\n","[278] epochs: 0\n","LOSS: -12.67124080657959\n","[278] epochs: 1\n","LOSS: -12.71336841583252\n","[278] epochs: 2\n","LOSS: -12.767301559448242\n","[278] epochs: 3\n","LOSS: -12.812761306762695\n","[278] epochs: 4\n","LOSS: -12.710367202758789\n","[278] epochs: 5\n","LOSS: -12.785798072814941\n","[278] epochs: 6\n","LOSS: -12.813180923461914\n","[278] epochs: 7\n","LOSS: -12.786885261535645\n","[278] epochs: 8\n","LOSS: -12.77578353881836\n","[278] epochs: 9\n","LOSS: -12.780546188354492\n","time_live: 73\n","cumulated reward: 49.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.954999923706055\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 279\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(52,), dtype=float32)\n","[279] epochs: 0\n","LOSS: 3.7132859230041504\n","time_live: 52\n","cumulated reward: -10.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.9679999351501465\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 280\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5  1.5  0.  -5. ], shape=(63,), dtype=float32)\n","[280] epochs: 0\n","LOSS: -4.280264854431152\n","time_live: 63\n","cumulated reward: 15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.889999866485596\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 281\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[281] epochs: 0\n","LOSS: 4.53160285949707\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.918000221252441\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 282\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[282] epochs: 0\n","LOSS: 3.084972858428955\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.921999931335449\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 283\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[283] epochs: 0\n","LOSS: -5.563716411590576\n","time_live: 62\n","cumulated reward: 19.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.830999851226807\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 284\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[284] epochs: 0\n","LOSS: -2.1161463260650635\n","time_live: 62\n","cumulated reward: 7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.7820000648498535\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 285\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[285] epochs: 0\n","LOSS: -3.7431437969207764\n","time_live: 62\n","cumulated reward: 13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.7129998207092285\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 286\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[286] epochs: 0\n","LOSS: 4.538101673126221\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.742000102996826\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 287\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[287] epochs: 0\n","LOSS: -3.7339582443237305\n","time_live: 62\n","cumulated reward: 13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.673999786376953\n","max_time_live: 73\n","max_cum_reward: 49.0\n","\n","EPISODE: 288\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5  1.5  1.5  1.5  1.5  2.5  2.5  2.5  2.5  3.5  2.5  2.5  2.5  2.5\n","  2.5  2.5  2.5  2.5  2.5  2.5  0.  -3.5], shape=(78,), dtype=float32)\n","[288] epochs: 0\n","LOSS: -15.852265357971191\n","[288] epochs: 1\n","LOSS: -15.858357429504395\n","[288] epochs: 2\n","LOSS: -15.878655433654785\n","[288] epochs: 3\n","LOSS: -15.910212516784668\n","[288] epochs: 4\n","LOSS: -15.949309349060059\n","[288] epochs: 5\n","LOSS: -15.991863250732422\n","[288] epochs: 6\n","LOSS: -16.03316307067871\n","[288] epochs: 7\n","LOSS: -15.987944602966309\n","[288] epochs: 8\n","LOSS: -16.035179138183594\n","[288] epochs: 9\n","LOSS: -16.101125717163086\n","time_live: 78\n","cumulated reward: 65.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.426000118255615\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 289\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[289] epochs: 0\n","LOSS: 3.621372938156128\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.438000202178955\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 290\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(46,), dtype=float32)\n","[290] epochs: 0\n","LOSS: 2.97082257270813\n","time_live: 46\n","cumulated reward: -7.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.441999912261963\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 291\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[291] epochs: 0\n","LOSS: 4.615457534790039\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.4710001945495605\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 292\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[292] epochs: 0\n","LOSS: 3.9613003730773926\n","time_live: 62\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.493000030517578\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 293\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[293] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.498000144958496\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 294\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(35,), dtype=float32)\n","[294] epochs: 0\n","LOSS: 2.3826184272766113\n","time_live: 35\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.493000030517578\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 295\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[295] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.514999866485596\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 296\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[296] epochs: 0\n","LOSS: -4.953069686889648\n","time_live: 62\n","cumulated reward: 17.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.435999870300293\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 297\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[297] epochs: 0\n","LOSS: 1.8725676536560059\n","time_live: 58\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.433000087738037\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 298\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n","  1.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[298] epochs: 0\n","LOSS: -0.2267112135887146\n","time_live: 62\n","cumulated reward: 1.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.4079999923706055\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 299\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[299] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.414999961853027\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 300\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[300] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.434000015258789\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 301\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[301] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.443999767303467\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 302\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[302] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.439000129699707\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 303\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[303] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.4670000076293945\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 304\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[304] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.486999988555908\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 305\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[305] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.514999866485596\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 306\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[306] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.526000022888184\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 307\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[307] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.520999908447266\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 308\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(33,), dtype=float32)\n","[308] epochs: 0\n","LOSS: 2.4726483821868896\n","time_live: 33\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.515999794006348\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 309\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[309] epochs: 0\n","LOSS: 4.426576614379883\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.539999961853027\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 310\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[310] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.560999870300293\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 311\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[311] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.565999984741211\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 312\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -5.5], shape=(46,), dtype=float32)\n","[312] epochs: 0\n","LOSS: 2.94142746925354\n","time_live: 46\n","cumulated reward: -7.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.568999767303467\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 313\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[313] epochs: 0\n","LOSS: 4.426576614379883\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5920000076293945\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 314\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[314] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5920000076293945\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 315\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[315] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.61899995803833\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 316\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[316] epochs: 0\n","LOSS: 2.6414589881896973\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.618000030517578\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 317\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[317] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.624000072479248\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 318\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[318] epochs: 0\n","LOSS: -2.522428512573242\n","time_live: 62\n","cumulated reward: 9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.574999809265137\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 319\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[319] epochs: 0\n","LOSS: 3.3286750316619873\n","time_live: 62\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.589000225067139\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 320\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5\n"," -0.5 -0.5  1.5  1.5  1.5  2.5  2.5  2.5  2.5  1.  -5. ], shape=(67,), dtype=float32)\n","[320] epochs: 0\n","LOSS: -7.746568202972412\n","time_live: 67\n","cumulated reward: 28.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.480999946594238\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 321\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[321] epochs: 0\n","LOSS: 1.502272605895996\n","time_live: 51\n","cumulated reward: -4.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.473999977111816\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 322\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[322] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.48799991607666\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 323\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[323] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.513999938964844\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 324\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[324] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.539999961853027\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 325\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[325] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.547999858856201\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 326\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[326] epochs: 0\n","LOSS: -0.7214687466621399\n","time_live: 62\n","cumulated reward: 3.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.51800012588501\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 327\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(32,), dtype=float32)\n","[327] epochs: 0\n","LOSS: 2.519651174545288\n","time_live: 32\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.513999938964844\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 328\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -5.5], shape=(42,), dtype=float32)\n","[328] epochs: 0\n","LOSS: 2.315284013748169\n","time_live: 42\n","cumulated reward: -5.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.511000156402588\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 329\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[329] epochs: 0\n","LOSS: 2.4865567684173584\n","time_live: 53\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.51200008392334\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 330\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[330] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5320000648498535\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 331\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[331] epochs: 0\n","LOSS: -4.342574596405029\n","time_live: 62\n","cumulated reward: 15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.4670000076293945\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 332\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -5.5], shape=(44,), dtype=float32)\n","[332] epochs: 0\n","LOSS: 2.641458749771118\n","time_live: 44\n","cumulated reward: -6.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.4670000076293945\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 333\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[333] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.4720001220703125\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 334\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[334] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.478000164031982\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 335\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[335] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.479000091552734\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 336\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[336] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.504000186920166\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 337\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(50,), dtype=float32)\n","[337] epochs: 0\n","LOSS: 3.474571466445923\n","time_live: 50\n","cumulated reward: -9.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.513000011444092\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 338\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[338] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.514999866485596\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 339\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[339] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.539999961853027\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 340\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[340] epochs: 0\n","LOSS: 3.3872241973876953\n","time_live: 62\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.552999973297119\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 341\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[341] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.547999858856201\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 342\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[342] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.550000190734863\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 343\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[343] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.573999881744385\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 344\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[344] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.584000110626221\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 345\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[345] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5879998207092285\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 346\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[346] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.59499979019165\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 347\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[347] epochs: 0\n","LOSS: 4.426576614379883\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.616000175476074\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 348\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[348] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.632999897003174\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 349\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -5.5], shape=(58,), dtype=float32)\n","[349] epochs: 0\n","LOSS: -1.2809419631958008\n","time_live: 58\n","cumulated reward: 4.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.60099983215332\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 350\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[350] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5970001220703125\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 351\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(36,), dtype=float32)\n","[351] epochs: 0\n","LOSS: 2.3395016193389893\n","time_live: 36\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5920000076293945\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 352\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[352] epochs: 0\n","LOSS: -2.522428512573242\n","time_live: 62\n","cumulated reward: 9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.547999858856201\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 353\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[353] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.572000026702881\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 354\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[354] epochs: 0\n","LOSS: 3.9474198818206787\n","time_live: 62\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.590000152587891\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 355\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[355] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.585999965667725\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 356\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(48,), dtype=float32)\n","[356] epochs: 0\n","LOSS: 3.2182648181915283\n","time_live: 48\n","cumulated reward: -8.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.591000080108643\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 357\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[357] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.605999946594238\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 358\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(49,), dtype=float32)\n","[358] epochs: 0\n","LOSS: 3.3488376140594482\n","time_live: 49\n","cumulated reward: -9.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.61299991607666\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 359\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5  1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[359] epochs: 0\n","LOSS: -4.342574596405029\n","time_live: 62\n","cumulated reward: 15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.552999973297119\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 360\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[360] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.547999858856201\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 361\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[361] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.565999984741211\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 362\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[362] epochs: 0\n","LOSS: 4.6023359298706055\n","time_live: 61\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.590000152587891\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 363\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -5.5], shape=(43,), dtype=float32)\n","[363] epochs: 0\n","LOSS: 2.481860876083374\n","time_live: 43\n","cumulated reward: -6.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5879998207092285\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 364\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[364] epochs: 0\n","LOSS: 1.596089482307434\n","time_live: 62\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.584000110626221\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 365\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[365] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5960001945495605\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 366\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -5.5], shape=(45,), dtype=float32)\n","[366] epochs: 0\n","LOSS: 2.7945146560668945\n","time_live: 45\n","cumulated reward: -7.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5970001220703125\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 367\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[367] epochs: 0\n","LOSS: 4.5316338539123535\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.619999885559082\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 368\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[368] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.642000198364258\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 369\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[369] epochs: 0\n","LOSS: 4.2392897605896\n","time_live: 57\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.658999919891357\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 370\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(36,), dtype=float32)\n","[370] epochs: 0\n","LOSS: 2.3395016193389893\n","time_live: 36\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.65500020980835\n","max_time_live: 78\n","max_cum_reward: 65.0\n","\n","EPISODE: 371\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5  1.5  1.5  2.5  2.5  2.5  2.5  3.5  2.5  2.5  2.5  2.5\n","  2.5  2.5  2.5  2.5  2.5  2.5  2.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(91,), dtype=float32)\n","[371] epochs: 0\n","LOSS: -6.515768051147461\n","[371] epochs: 1\n","LOSS: -6.5159831047058105\n","[371] epochs: 2\n","LOSS: -6.52343225479126\n","[371] epochs: 3\n","LOSS: -6.537010192871094\n","[371] epochs: 4\n","LOSS: -6.556636810302734\n","[371] epochs: 5\n","LOSS: -6.582895278930664\n","[371] epochs: 6\n","LOSS: -6.610536575317383\n","[371] epochs: 7\n","LOSS: -6.635413646697998\n","[371] epochs: 8\n","LOSS: -6.618155479431152\n","[371] epochs: 9\n","LOSS: -6.58903169631958\n","time_live: 91\n","cumulated reward: 31.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.553999900817871\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 372\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -5.5], shape=(59,), dtype=float32)\n","[372] epochs: 0\n","LOSS: 4.426576137542725\n","time_live: 59\n","cumulated reward: -14.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.573999881744385\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 373\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(61,), dtype=float32)\n","[373] epochs: 0\n","LOSS: 4.6023359298706055\n","time_live: 61\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5960001945495605\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 374\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[374] epochs: 0\n","LOSS: 3.9506278038024902\n","time_live: 62\n","cumulated reward: -13.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.61299991607666\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 375\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[375] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.627999782562256\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 376\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[376] epochs: 0\n","LOSS: -0.12594930827617645\n","time_live: 62\n","cumulated reward: 1.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.60699987411499\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 377\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n","  1.5  1.5  1.5  1.5  1.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[377] epochs: 0\n","LOSS: 1.0571132898330688\n","time_live: 62\n","cumulated reward: -3.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.5980000495910645\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 378\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[378] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.611999988555908\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 379\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(54,), dtype=float32)\n","[379] epochs: 0\n","LOSS: 3.9341487884521484\n","time_live: 54\n","cumulated reward: -11.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.625\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 380\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[380] epochs: 0\n","LOSS: 3.0825650691986084\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.629000186920166\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 381\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0. -5.], shape=(41,), dtype=float32)\n","[381] epochs: 0\n","LOSS: 2.141251802444458\n","time_live: 41\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.624000072479248\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 382\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -5.5], shape=(47,), dtype=float32)\n","[382] epochs: 0\n","LOSS: 3.0825653076171875\n","time_live: 47\n","cumulated reward: -8.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.627999782562256\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 383\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5  1.5  1.5  1.5  1.5  1.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[383] epochs: 0\n","LOSS: 1.5191969871520996\n","time_live: 62\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.624000072479248\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 384\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(53,), dtype=float32)\n","[384] epochs: 0\n","LOSS: 3.8252995014190674\n","time_live: 53\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.635000228881836\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 385\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -5.], shape=(33,), dtype=float32)\n","[385] epochs: 0\n","LOSS: 2.4726483821868896\n","time_live: 33\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.63100004196167\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 386\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(56,), dtype=float32)\n","[386] epochs: 0\n","LOSS: 4.140965461730957\n","time_live: 56\n","cumulated reward: -12.5\n","avg_time_live: 53.0\n","avg_cum_reward: -6.645999908447266\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 387\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(55,), dtype=float32)\n","[387] epochs: 0\n","LOSS: 4.039309024810791\n","time_live: 55\n","cumulated reward: -12.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.659999847412109\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 388\n","tf.Tensor(\n","[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n","  0.  0.  0. -5.], shape=(40,), dtype=float32)\n","[388] epochs: 0\n","LOSS: 2.1787190437316895\n","time_live: 40\n","cumulated reward: -5.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.656000137329102\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 389\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -5.5], shape=(57,), dtype=float32)\n","[389] epochs: 0\n","LOSS: 3.618927001953125\n","time_live: 57\n","cumulated reward: -11.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.666999816894531\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 390\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -5.5], shape=(51,), dtype=float32)\n","[390] epochs: 0\n","LOSS: 3.595731735229492\n","time_live: 51\n","cumulated reward: -10.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.675000190734863\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 391\n","tf.Tensor(\n","[ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n","  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5 -0.5\n"," -0.5 -0.5 -0.5 -0.5 -0.5 -5. ], shape=(62,), dtype=float32)\n","[391] epochs: 0\n","LOSS: 4.531634330749512\n","time_live: 62\n","cumulated reward: -15.0\n","avg_time_live: 53.0\n","avg_cum_reward: -6.696000099182129\n","max_time_live: 91\n","max_cum_reward: 65.0\n","\n","EPISODE: 392\n"]}],"source":["# tf.random.set_seed(1)\n","\n","Agent = Actor\n","\n","NUM_EPISODE = 50000\n","GAMMA = 0.95\n","EXPLORE_RATIO_STAGE1 = 0.85\n","EXPLORE_RATIO_STAGE2 = 0.9\n","EXPLORE_LIMIT_STAGE1 = 5000 \n","EXPLORE_LIMIT_STAGE2 = 8000\n","EXPLORE_LIMIT_CEILING = 10000\n","START_PPO_UPDATE = 60 \n","EPOCHS = 10\n","\n","best = 200\n","Cum_reward = []\n","Ts = []\n","\n","iter_num = 0\n","for episode in range(0, NUM_EPISODE + 1, 1):\n","\n","    # Reset the environment\n","    env.reset_game()\n","\n","    frames = [env.getScreenRGB()]\n","    \n","    cum_reward = 0\n","  \n","    # all_aloss = []\n","    # all_closs = []\n","    rewards = []\n","    states = []\n","    actions = []\n","    old_probs = []\n","    # values = []\n","\n","    # feed current state and select an action\n","    state = tf.constant(np.array(TA_state()).reshape(1,8))\n","    states.append(state)\n","\n","    T = 0\n","    print(\"EPISODE: {}\".format(episode))\n","    while not env.game_over():\n","\n","        # feed current state and select an action\n","        Stochastic = Agent(state)[0].numpy()\n","\n","        # Exploration\n","        if episode < EXPLORE_LIMIT_STAGE1:\n","            if Stochastic[0] > EXPLORE_RATIO_STAGE1:\n","                Stochastic[0] = EXPLORE_RATIO_STAGE1\n","                Stochastic[1] = 1 - EXPLORE_RATIO_STAGE1\n","            elif Stochastic[0] < 1-EXPLORE_RATIO_STAGE1:\n","                Stochastic[0] = 1 - EXPLORE_RATIO_STAGE1\n","                Stochastic[1] = EXPLORE_RATIO_STAGE1\n","\n","        if episode >= EXPLORE_LIMIT_STAGE2 & episode < EXPLORE_LIMIT_CEILING:\n","            if Stochastic[0] > EXPLORE_RATIO_STAGE2:\n","                Stochastic[0] = EXPLORE_RATIO_STAGE2\n","                Stochastic[1] = 1 - EXPLORE_RATIO_STAGE2\n","            elif Stochastic[0] < 1-EXPLORE_RATIO_STAGE2:\n","                Stochastic[0] = 1 - EXPLORE_RATIO_STAGE2\n","                Stochastic[1] = EXPLORE_RATIO_STAGE2\n","        \n","        action = np.random.choice(2 ,p =  Stochastic)\n","        prob = Stochastic[action]\n","        # value = Agent.critic(state).numpy()\n","\n","        # execute the action and get reward\n","        reward = env.act(env.getActionSet()[action])\n","\n","        frames.append(env.getScreenRGB())\n","        \n","        # collect trajectory\n","        actions.append(action)\n","        rewards.append(reward) \n","        old_probs.append(prob)\n","        # values.append(value)\n","        \n","        state = np.array(TA_state()).reshape(1,8)\n","        states.append(state)\n","        \n","        if T>500 and T%100 == 0:\n","            print(\"T_IN_TRAJECTORY: {}\".format(T))\n","        T += 1\n","\n","    if T>500:\n","        print(\"MAX_T_BEFORE_PPO_STAGE2: {}\".format(T))\n","    \n","    # value = Agent.critic(state).numpy()\n","    # values.append(value)\n","\n","    # print(states)\n","\n","    Rewards = Reward_func(states, rewards)\n","    cum_reward = np.sum(Rewards)\n","    states = tf.constant(np.array(states[:-1]).reshape(len(states[:-1]),8) )   # [[[],[]]]\n","    actions = np.array(actions, dtype=np.int32)\n","    Cum_reward.append(np.round(cum_reward,3))\n","    Ts.append(T)\n","    \n","\n","    print(Rewards)\n","    # CALCULATE ADVANTAGE BASED ON THE NEW REWARDS\n","    adv = Advantage_func_fromback(Rewards, len(Rewards), GAMMA)\n","\n","    # print(\"EPISODE: \",episode,'\\n')\n","    # print(\"STATES: \",states,'\\n')\n","    # print(\"PROBS: \",old_probs,'\\n')\n","    # print(\"ACTIONS: \",actions,'\\n')\n","    # print(\"REWARDS: \",rewards,'\\n')\n","    # print(\"NEW REWARDS: \",Rewards,'\\n')\n","    # print(\"ADVANTAGE: \", adv,'\\n')\n","    # print(\"CUM REWARDS: \", cum_reward,'\\n')\n","\n","    if T <= START_PPO_UPDATE:\n","        actor_loss = train_step(states, actions, adv, old_probs, episode)\n","        print(\"[{}] epochs: {}\".format(episode, 0))\n","        print(\"LOSS: {}\".format(actor_loss))\n","    else: \n","        for epochs in range(EPOCHS):\n","            actor_loss = train_step(states, actions, adv, old_probs, episode)\n","            print(\"[{}] epochs: {}\".format(episode, epochs))\n","            print(\"LOSS: {}\".format(actor_loss))\n","        START_PPO_UPDATE = T\n","\n","    print(\n","        \"time_live: {}\\ncumulated reward: {}\\navg_time_live: {}\\navg_cum_reward: {}\\nmax_time_live: {}\\nmax_cum_reward: {}\\n\".\n","        format(T, np.round(cum_reward,3), np.round(np.mean(Ts)), np.round(np.mean(Cum_reward),3), np.max(Ts), np.round(np.max(Cum_reward),3)))\n","\n","    if (T>best):\n","        print('\\n')\n","        # Agent.save('test{}.h5'.format(T))\n","        tf.keras.models.save_model(Agent, filepath='./model_rewardsum{}/'.format(T))\n","        print('\\n')\n","        clip = make_anim(frames, fps=60, true_image=True).rotate(-90)\n","        clip.write_videofile(\"/home/ingmember03/DL2020/DL2020_07/comp4/PPO_rewardsum_demo-{}.webm\".format(T), fps=60)\n","#         display(clip.ipython_display(fps=60, autoplay=1, loop=1, maxduration=120))\n","        best = T\n","\n","# pipe1 -> 62 - 78\n","# pipe2 -> 98 - 114\n","# pipe3 -> 134 - 150"]},{"cell_type":"markdown","metadata":{"id":"xs4c2fz4MZXd"},"source":["# 2. A3C - state-based"]},{"cell_type":"markdown","metadata":{"id":"anuwV0UT7Ie9"},"source":["### Parameters\n","Here we define parameters used in A3C. Also, we change the original rewards in the game."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"av2QJOa36ohm"},"outputs":[],"source":["args = {\n","  'gamma' : 0.9,\n","  'update_interval':300,\n","  'actor_lr':0.001,\n","  'critic_lr':0.001,\n","  'entropy_beta':0.05,\n","  'reward_no_die':0.01,\n","  'reward_die':-5,\n","  'reward_through':1.5\n","}\n","CUR_EPISODE = 0"]},{"cell_type":"markdown","metadata":{"id":"0NkBLOeP78jV"},"source":["### Define Actor and Critic model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L1zWfIRl731L"},"outputs":[],"source":["class Actor:\n","    def __init__(self, state_dim, action_dim):\n","        self.state_dim = state_dim\n","        self.action_dim = action_dim\n","        self.model = self.create_model()\n","        self.opt = tf.keras.optimizers.Adam(args['actor_lr'])\n","        self.entropy_beta = args['entropy_beta']\n","\n","    def create_model(self):\n","        return tf.keras.Sequential([\n","            Input((self.state_dim,)),\n","            Dense(256, activation='relu'),\n","            Dense(128, activation='relu'),\n","            Dense(64, activation='relu'),\n","            Dense(self.action_dim, activation='softmax')\n","        ])\n","\n","    def compute_loss(self, actions, logits, advantages):\n","        ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","            from_logits=True)\n","        entropy_loss = tf.keras.losses.CategoricalCrossentropy(\n","            from_logits=True)\n","        actions = tf.cast(actions, tf.int32)\n","        policy_loss = ce_loss(\n","            actions, logits, sample_weight=tf.stop_gradient(advantages))\n","        entropy = entropy_loss(logits, logits)\n","        return policy_loss - self.entropy_beta * entropy\n","\n","    def train(self, states, actions, advantages):\n","        with tf.GradientTape() as tape:\n","            logits = self.model(states, training=True)\n","            loss = self.compute_loss(\n","                actions, logits, advantages)\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n","        return loss\n","\n","class Critic:\n","    def __init__(self, state_dim):\n","        self.state_dim = state_dim\n","        self.model = self.create_model()\n","        self.opt = tf.keras.optimizers.Adam(args['critic_lr'])\n","\n","    def create_model(self):\n","        return tf.keras.Sequential([\n","            Input((self.state_dim,)),\n","            Dense(256, activation='relu'),\n","            Dense(128, activation='relu'),\n","            Dense(64, activation='relu'),\n","            Dense(1, activation='linear')\n","        ])\n","\n","    def compute_loss(self, v_pred, td_targets):\n","        mse = tf.keras.losses.MeanSquaredError()\n","        return mse(td_targets, v_pred)\n","\n","    def train(self, states, td_targets):\n","        with tf.GradientTape() as tape:\n","            v_pred = self.model(states, training=True)\n","            assert v_pred.shape == td_targets.shape\n","            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n","        grads = tape.gradient(loss, self.model.trainable_variables)\n","        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"k5lrX6fy8KNB"},"source":["### Define Global Agent\n","In A3C, we have to define a global model, which collect games played from all threads in CPU and then update the global model parametors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_h6Ik2vF8Yqi"},"outputs":[],"source":["class Agent:\n","    def __init__(self, env_name):\n","        env = make_new()\n","        self.env_name = env_name\n","        self.state_dim = TA_state(env).shape[1]\n","        self.action_dim = len(env.getActionSet())\n","\n","        self.global_actor = Actor(self.state_dim, self.action_dim)\n","        self.global_critic = Critic(self.state_dim)\n","        self.num_workers = cpu_count()\n","        #self.num_workers = 1\n","    def train(self, max_episodes=1000000):\n","        workers = []\n","        for i in range(self.num_workers):\n","            env = make_new()\n","            workers.append(WorkerAgent(\n","                env, self.global_actor, self.global_critic, max_episodes))\n","\n","        for worker in workers:\n","            worker.start()\n","\n","        for worker in workers:\n","            worker.join()"]},{"cell_type":"markdown","metadata":{"id":"h2zCua-g9UHN"},"source":["### Define Local Agent\n","This agent is used to play in threads. Note that we update the global parametors once after collecting 300 games."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okzJRQHp90Px"},"outputs":[],"source":["class WorkerAgent(Thread):\n","    def __init__(self, env, global_actor, global_critic, max_episodes):\n","        Thread.__init__(self)\n","        self.lock = Lock()\n","        self.env = env\n","        self.state_dim = TA_state(self.env).shape[1]\n","        self.action_dim = len(self.env.getActionSet())\n","\n","        self.max_episodes = max_episodes\n","        self.global_actor = global_actor\n","        self.global_critic = global_critic\n","        self.actor = Actor(self.state_dim, self.action_dim)\n","        self.critic = Critic(self.state_dim)\n","\n","        self.actor.model.set_weights(self.global_actor.model.get_weights())\n","        self.critic.model.set_weights(self.global_critic.model.get_weights())\n","\n","    def n_step_td_target(self, rewards, next_v_value, done):\n","        td_targets = np.zeros_like(rewards)\n","        cumulative = 0\n","        if not done:\n","            cumulative = next_v_value\n","\n","        for k in reversed(range(0, len(rewards))):\n","            cumulative = args['gamma'] * cumulative + rewards[k]\n","            td_targets[k] = cumulative\n","        return td_targets\n","\n","    def advatnage(self, td_targets, baselines):\n","        return td_targets - baselines\n","\n","    def list_to_batch(self, list):\n","        batch = list[0]\n","        for elem in list[1:]:\n","            batch = np.append(batch, elem, axis=0)\n","        return batch\n","\n","    def train(self):\n","        global CUR_EPISODE\n","\n","        while self.max_episodes >= CUR_EPISODE:\n","            state_batch = []\n","            action_batch = []\n","            reward_batch = []\n","            episode_reward, done = 0, False\n","            self.env.reset_game()\n","\n","            state = TA_state(self.env)\n","            total_loss =0\n","            while not done:\n","                probs = self.actor.model.predict(state)\n","                action = np.random.choice(self.action_dim, p=probs[0])\n","\n","                reward = reward_trans(self.env.act(self.env.getActionSet()[action])) \n","                \n","                next_state = TA_state(self.env)\n","                done = self.env.game_over()\n","\n","                action = np.reshape(action, [1, 1])\n","                reward = np.reshape(reward, [1, 1])\n","\n","                state_batch.append(state)\n","                action_batch.append(action)\n","                reward_batch.append(reward)\n","\n","                if len(state_batch) >= args['update_interval'] or done:\n","                    states = self.list_to_batch(state_batch)\n","                    actions = self.list_to_batch(action_batch)\n","                    rewards = self.list_to_batch(reward_batch)\n","\n","                    next_v_value = self.critic.model.predict(next_state)\n","                    td_targets = self.n_step_td_target(\n","                        rewards, next_v_value, done)\n","                    advantages = td_targets - self.critic.model.predict(states)\n","                    \n","                    with self.lock:\n","                        actor_loss = self.global_actor.train(\n","                            states, actions, advantages)\n","                        critic_loss = self.global_critic.train(\n","                            states, td_targets)\n","\n","                        self.actor.model.set_weights(\n","                            self.global_actor.model.get_weights())\n","                        self.critic.model.set_weights(\n","                            self.global_critic.model.get_weights())\n","                    \n","                    total_loss+=actor_loss\n","                    total_loss+=critic_loss\n","\n","                    state_batch = []\n","                    action_batch = []\n","                    reward_batch = []\n","                    td_target_batch = []\n","                    advatnage_batch = []\n","\n","                episode_reward += reward[0][0]\n","                state = next_state\n","            if CUR_EPISODE % 100 == 0:    \n","                print('EP{} EpisodeReward={} TotalLoss={}\\n'.format(CUR_EPISODE, episode_reward,total_loss))\n","            wandb.log({'Reward': episode_reward,'Total Loss':total_loss})\n","            CUR_EPISODE += 1"]},{"cell_type":"markdown","metadata":{"id":"CIhS5ZP5-oEz"},"source":["### Start training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juRsBNuk-szi"},"outputs":[],"source":["agent = Agent(\"flappy_bird_A3C\")\n","agent.train(100000)"]},{"cell_type":"markdown","metadata":{"id":"ue1cE34DMaGj"},"source":["# 3. PG - state-based"]},{"cell_type":"markdown","metadata":{"id":"smGtlfKVd379"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zc_JClOUd3m1"},"outputs":[],"source":["import numpy as np\r\n","from tensorflow.keras.models import Sequential\r\n","from tensorflow.keras.layers import Dense, Reshape, Flatten\r\n","from tensorflow.keras.optimizers import Adam\r\n","from IPython.display import Image, display\r\n","import moviepy.editor as mpy\r\n","import tensorflow as tf\r\n","import os\r\n","os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\r\n","import copy\r\n","from ple.games.flappybird import FlappyBird\r\n","from ple import PLE\r\n","\r\n","multiple_return_values = False\r\n","gpu_number = 0\r\n","seed = 2021\r\n","\r\n","gpus = tf.config.experimental.list_physical_devices('GPU')\r\n","if gpus:\r\n","    try:\r\n","        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\r\n","        # Currently, memory growth needs to be the same across GPUs\r\n","        for gpu in gpus:\r\n","            tf.config.experimental.set_memory_growth(gpu, True)\r\n","        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\r\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\r\n","    except RuntimeError as e:\r\n","        # Memory growth must be set before GPUs have been initialized\r\n","        print(e)\r\n","\r\n","game = FlappyBird()\r\n","env = PLE(game, fps=30, display_screen=False, rng=seed)  # game environment interface\r\n","env.reset_game()\r\n","\r\n","def TA_state():\r\n","    state = copy.deepcopy(game.getGameState())\r\n","    \r\n","    state['next_next_pipe_bottom_y'] -= state['player_y']\r\n","    state['next_next_pipe_top_y'] -= state['player_y']\r\n","    state['next_pipe_bottom_y'] -= state['player_y']\r\n","    state['next_pipe_top_y'] -= state['player_y']\r\n","    relative_state = list(state.values())\r\n","\r\n","\r\n","    # return the state in tensor type, with batch dimension\r\n","    relative_state = tf.convert_to_tensor(relative_state, dtype=tf.float32)\r\n","    relative_state = tf.expand_dims(relative_state, axis=0)\r\n","    \r\n","    return relative_state\r\n","\r\n","def MY_reward(n,p):\r\n","\r\n","    a = n['next_pipe_bottom_y']\r\n","    b = n['next_pipe_top_y']\r\n","    \r\n","    re_n = (a+b)/2\r\n","    re_n -= n['player_y']\r\n","    \r\n","    a = p['next_pipe_bottom_y']\r\n","    b = p['next_pipe_top_y']\r\n","    \r\n","    re_p = (a+b)/2\r\n","    re_p -= p['player_y']\r\n","\r\n","    \r\n","    re_n = -(np.absolute(re_n))\r\n","    re_p = -(np.absolute(re_p))\r\n","\r\n","    return (re_n - re_p)/16\r\n","  \r\n","model = tf.keras.Sequential()\r\n","model.add(tf.keras.layers.Dense(32, input_dim = 8, activation='relu', kernel_initializer='random_normal'))\r\n","model.add(tf.keras.layers.Dense(32, activation='relu', kernel_initializer='random_normal'))\r\n","model.add(tf.keras.layers.Dense(32, activation='relu', kernel_initializer='random_normal'))\r\n","model.add(tf.keras.layers.Dense(2, activation = \"softmax\"))\r\n","model.build()\r\n","optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\r\n","compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n","print(model.summary())\r\n","\r\n","def discount_rewards(r, gamma = 0.5):\r\n","    discounted_r = np.zeros_like(r)\r\n","    running_add = 0\r\n","    for t in reversed(range(0, r.size)):\r\n","        running_add = running_add * gamma + r[t]\r\n","        discounted_r[t] = running_add\r\n","    return discounted_r\r\n","\r\n","class GradUpdate:\r\n","    def __init__(self, model):\r\n","        self.Buffer = model.trainable_variables\r\n","        self.zero()\r\n","        \r\n","    def zero(self):\r\n","        for ix, grad in enumerate(self.Buffer):\r\n","            self.Buffer[ix] = grad * 0\r\n","            \r\n","    def update(self, ep_memory):\r\n","        for grads, r in ep_memory:\r\n","            for ix, grad in enumerate(grads):\r\n","                self.Buffer[ix] += grad * r\r\n","                \r\n","def get_action(model, s):\r\n","    s = s.reshape([1,4])\r\n","    logits = model(s)\r\n","    a_dist = logits.numpy()\r\n","    # Choose random action with p = action dist\r\n","    a = np.random.choice(a_dist[0],p=a_dist[0])\r\n","    a = np.argmax(a_dist == a)    \r\n","    return logits, a\r\n","\r\n","episodes = 2000\r\n","scores = []\r\n","update_every = 5\r\n","gradBuffer = GradUpdate(model)  \r\n","\r\n","h = 0\r\n","for e in range(episodes):\r\n","    env.reset_game()\r\n","    \r\n","    frames = [env.getScreenRGB()]\r\n","    \r\n","    ep_memory = []\r\n","    ep_score = 0\r\n","    done = False \r\n","    t = 0\r\n","    s_n = game.getGameState()\r\n","    \r\n","    while not env.game_over(): \r\n","        with tf.GradientTape() as tape:\r\n","            #forward pass\r\n","            state = TA_state()\r\n","            logits = model(state)\r\n","            a_dist = logits.numpy()\r\n","            #print(a_dist)\r\n","            # Choose random action with p = action dist\r\n","            action = np.random.choice(a_dist[0],p=a_dist[0])\r\n","            action = np.argmax(a_dist == action)                    \r\n","            loss = compute_loss([action], logits)\r\n","        # make the choosen action \r\n","\r\n","        reward = env.act(env.getActionSet()[action])\r\n","        frames.append(env.getScreenRGB())\r\n","\r\n","        ep_score +=reward\r\n","        \r\n","        s_p = s_n\r\n","        s_n = game.getGameState()\r\n","        reward += MY_reward(s_n, s_p)\r\n","\r\n","        grads = tape.gradient(loss, model.trainable_variables)\r\n","        ep_memory.append([grads,reward])\r\n","        scores.append(ep_score)\r\n","        t+=1\r\n","    \r\n","    if(t>h):\r\n","        model.save('test{}.h5'.format(t))\r\n","        h = t\r\n","    # Discound the rewards \r\n","    ep_memory = np.array(ep_memory)\r\n","    ep_memory[:,1] = discount_rewards(ep_memory[:,1])    \r\n","    \r\n","    gradBuffer.update(ep_memory)        \r\n","    if e % update_every == 0:\r\n","        optimizer.apply_gradients(zip(gradBuffer.Buffer, model.trainable_variables))\r\n","        gradBuffer.zero()\r\n","\r\n","    if e % 100 == 0:\r\n","        print(\"Episode  {}  Score  {}\".format(e, np.mean(scores[-100:])))\r\n","    print(t)"]},{"cell_type":"markdown","metadata":{"id":"cBVoC3uwEQvL"},"source":["# 4. Conclusion"]},{"cell_type":"markdown","metadata":{"id":"3mwQCXa9EYx-"},"source":["- After trying above 3 models, we find out that although original policy gradients quite simple, its performance is still good comparing to others.\r\n","- While trying PPO (off-policy), we find that it's not necessary to update more than 1 time in every trajectory, we can just focus on longer trajectories and update many times during training. In addition, during the early epsiodes, it is also not necessary to launch several updates per trajectory, since it doesn't take much time to collect trajectory in the early stage.\r\n","- We includes state information to define two different reward functions:\r\n","  1. based on the bird height comparing to the middle point of pipe, therefore, we can create a continuous reward function.\r\n","  2. based on the distance of the bird to pipe, for example, if bird is 20 time steps away from the pipe, if it flies over or lower than the pipe's top or bottom, we penalize it, o.w., we reward.\r\n","- For a well-defined reward function, we suggest that the baseline in advantage function can be removed since rewards are in both positive and negative already.\r\n","- While building model, we find out that leaky relu activation function is better than relu."]}],"metadata":{"colab":{"collapsed_sections":["Jeo3-FO-6fJm","GNsuE0M_7aAN","mXC676Wh7c06","vIgSeuma8Pbh","OZ0o7HMMJdLF","_ha9dYcI6fJx","p_2MgaBrJj2x","VZtzkpwF6fJq","q_iyPlB46fJr","M107PKOf6fJs","05hwVC_x6fJt","AamX9Jwa6fJt","gq5SUb-KA8t1","gpErlGvH6fJv","9TVuVu1-6fJw","XhdOpRbL6fJx","sO01NKZC6fJz","_3SgCtG06fJz","xs4c2fz4MZXd","anuwV0UT7Ie9","0NkBLOeP78jV","k5lrX6fy8KNB","h2zCua-g9UHN","CIhS5ZP5-oEz","smGtlfKVd379"],"name":"Reinforcement Learning - Flappy Bird.ipynb","provenance":[{"file_id":"1vH9jm72nHpEKUa2dG__5mbqjIeQmz4s5","timestamp":1611070205868}]},"kernelspec":{"display_name":"Python 3.7.9 64-bit ('bachi': conda)","name":"python379jvsc74a57bd09ebc6147973a05ce4bbd06f13fbfeff7bb540ef572b036e6c34da4f1ef7b2b65"},"language_info":{"name":"python","version":""},"orig_nbformat":2},"nbformat":4,"nbformat_minor":0}